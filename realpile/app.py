import streamlit as st
import cv2
import time
import threading

import cv2
import mediapipe as mp
import time
import numpy as np
import joblib
import keyboard  # q ì…ë ¥ìœ¼ë¡œ ì¢…ë£Œ í™•ì¸

# -------------------------------
# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ
# -------------------------------
try:
    MODEL_FILE = "./DAiSEE/models/focus_model.pkl"
    if 'model' not in st.session_state:
        st.session_state['model'] = joblib.load(MODEL_FILE)
except FileNotFoundError:
    st.error(f"ëª¨ë¸ íŒŒì¼ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”: {MODEL_FILE}")
    st.session_state['model'] = None

# -------------------------------
# Mediapipe ì´ˆê¸°í™”
# -------------------------------
mp_holistic = mp.solutions.holistic

# -------------------------------
# ë¯¼ê°ë„ ê¸°ì¤€ ì„¤ì •
# -------------------------------
CLOSED_THRESHOLD = 3         # ëˆˆ ì™„ì „íˆ ê°ê¹€ ê¸°ì¤€
HALF_CLOSED_THRESHOLD = 6    # ëˆˆ ë°˜ê°ê¹€ ê¸°ì¤€
YAWN_THRESHOLD = 25          # í•˜í’ˆ ê¸°ì¤€
GAZE_THRESHOLD = 0.45        # ì‹œì„  ì´íƒˆ ê¸°ì¤€
BLINK_MAX = 20               # ìµœëŒ€ ê¹œë¹¡ì„ ìˆ˜

# -------------------------------
# ì§‘ì¤‘ë„ ê³„ì‚° í•¨ìˆ˜
# -------------------------------
def calculate_focus(yawn, blink, closed_time, half_closed_time, gaze_out_time,
                    w1=0.3, w2=0.2, w3=0.3, w4=0.2):
    """
    ê° í–‰ë™ë³„ ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ íŒ¨ë„í‹° ê³„ì‚° í›„ 100ì—ì„œ ì°¨ê°
    """
    penalty = 100 * (w1*yawn + w2*(blink/BLINK_MAX) + w3*(closed_time/10) + w4*(gaze_out_time/10))
    score = 100 - penalty
    return max(0, score)

# -------------------------------
# ì§‘ì¤‘ë„ ì €í•˜ ì´ìœ  ë¶„ì„
# -------------------------------
def analyze_focus_reason(blink, yawn, closed, half_closed, gaze):
    """
    í‰ê· ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ë†’ì€ íŒ¨ë„í‹° í•­ëª©ì„ ì›ì¸ìœ¼ë¡œ íŒë‹¨
    """
    penalties = {
        'blink': blink * 0.2,
        'yawn': yawn * 0.3,
        'closed': closed * 0.3,
        'half_closed': half_closed * 0.1,
        'gaze_out': gaze * 0.2
    }
    reason = max(penalties, key=penalties.get)
    return reason, penalties

# -------------------------------
# ì›¹ìº  ì„¤ì •
# -------------------------------
cap = cv2.VideoCapture(0)

# -------------------------------
# ì¸¡ì • ë³€ìˆ˜ ì´ˆê¸°í™”
# -------------------------------
segment_start = time.time()
all_scores = []
all_states = []

blink_count = 0
yawn_count = 0
closed_seconds = 0
half_closed_seconds = 0
gaze_out_seconds = 0
eye_closed = False
yawn_state = False

# -------------------------------
# í‰ê·  ì§‘ì¤‘ë„ ê³„ì‚° ë° ë‚®ì€ ì›ì¸ ë¶„ì„
# -------------------------------
if all_scores:
    avg_focus = round(sum(all_scores)/len(all_scores), 2)

    avg_blink = sum(s['blink_count'] for s in all_states)/len(all_states)
    avg_yawn = sum(s['yawn_count'] for s in all_states)/len(all_states)
    avg_closed = sum(s['closed_seconds'] for s in all_states)/len(all_states)
    avg_half_closed = sum(s['half_closed_seconds'] for s in all_states)/len(all_states)
    avg_gaze = sum(s['gaze_out_seconds'] for s in all_states)/len(all_states)

    reason, penalties = analyze_focus_reason(avg_blink, avg_yawn, avg_closed, avg_half_closed, avg_gaze)

    print(f"\nìµœì¢… í‰ê·  ì§‘ì¤‘ë„: {avg_focus}")
    print(f"ì§‘ì¤‘ë„ê°€ ë‚®ì€ ì£¼ ì›ì¸: {reason}")
    print(f"ìƒì„¸ íŒ¨ë„í‹°: {penalties}")
else:
    print("ì§‘ì¤‘ë„ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

# ===============================
#  ê·¸ë˜í”„ ì‹œê°í™” (matplotlib)
# ===============================
import matplotlib
matplotlib.use('TkAgg')     # OpenCV ì¶©ëŒ ë°©ì§€
import matplotlib.pyplot as plt

if all_scores:
    # ---------- â‘  ì‹œê°„ íë¦„ì— ë”°ë¥¸ ì§‘ì¤‘ë„ ê·¸ë˜í”„ ----------
    plt.figure(figsize=(8, 4))
    plt.plot(range(1, len(all_scores) + 1), all_scores, marker='o', linestyle='-', linewidth=2)
    plt.title("ì‹œê°„ íë¦„ì— ë”°ë¥¸ ì§‘ì¤‘ë„ ë³€í™”")
    plt.xlabel("ì¸¡ì • êµ¬ê°„ (10ì´ˆ ë‹¨ìœ„)")
    plt.ylabel("ì§‘ì¤‘ë„ ì ìˆ˜")
    plt.ylim(0, 100)
    plt.grid(True)
    plt.tight_layout()
    plt.show(block=True)

    # ---------- â‘¡ íŒ¨ë„í‹° ë§‰ëŒ€ê·¸ë˜í”„ ----------
    labels = list(penalties.keys())
    values = list(penalties.values())
    main_cause = max(penalties, key=penalties.get)

    plt.figure(figsize=(8, 5))
    bars = plt.bar(labels, values)

    # ì›ì¸ ë§‰ëŒ€ ê°•ì¡°
    main_index = labels.index(main_cause)
    bars[main_index].set_edgecolor("red")
    bars[main_index].set_linewidth(3)

    # ë§‰ëŒ€ ìœ„ ê°’ í‘œì‹œ
    for i, v in enumerate(values):
        plt.text(i, v + 0.02, f"{v:.2f}", ha='center')

    plt.title(f" ì§‘ì¤‘ë„ íŒ¨ë„í‹° ë¶„ì„ (í‰ê·  ì§‘ì¤‘ë„: {avg_focus})")
    plt.xlabel("í–‰ë™ ìš”ì†Œ")
    plt.ylabel("íŒ¨ë„í‹° í¬ê¸°")
    plt.ylim(0, max(values) + 0.2)
    plt.tight_layout()
    plt.show(block=True)

    print("ê·¸ë˜í”„ ì¶œë ¥ ì™„ë£Œ.")



def statistics_start(cap, stop_flag, model):
    with mp_holistic.Holistic(
        static_image_mode=False,
        model_complexity=1,
        refine_face_landmarks=True
    ) as holistic:
        try:
            while not stop_flag.is_set():
                ret, frame = cap.read()
                if not ret:
                    print("í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                h, w, _ = frame.shape
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = holistic.process(rgb)

                # Streamlit Session Stateì—ì„œ ë³€ìˆ˜ ë¡œë“œ
                blink_count = st.session_state['blink_count']
                yawn_count = st.session_state['yawn_count']
                closed_seconds = st.session_state['closed_seconds']
                half_closed_seconds = st.session_state['half_closed_seconds']
                gaze_out_seconds = st.session_state['gaze_out_seconds']
                eye_closed = st.session_state['eye_closed']
                yawn_state = st.session_state['yawn_state']
                segment_start = st.session_state['segment_start']

                if results.face_landmarks:
                    landmarks = results.face_landmarks.landmark

                    # --- ëˆˆ ê¹œë¹¡ì„ ê³„ì‚° ---
                    left_eye = abs(landmarks[145].y - landmarks[159].y) * h
                    right_eye = abs(landmarks[374].y - landmarks[386].y) * h
                    eye_avg = (left_eye + right_eye) / 2

                    if eye_avg < CLOSED_THRESHOLD:
                        closed_seconds += 1/30.0 # í”„ë ˆì„ë‹¹ ì‹œê°„ ëˆ„ì 
                        if not eye_closed:
                            eye_closed = True
                            blink_count += 1
                    else:
                        eye_closed = False

                    if CLOSED_THRESHOLD <= eye_avg < HALF_CLOSED_THRESHOLD:
                        half_closed_seconds += 1/30.0

                    # --- í•˜í’ˆ ê³„ì‚° ---
                    lip_dist = abs(landmarks[13].y - landmarks[14].y) * h
                    if lip_dist > YAWN_THRESHOLD:
                        if not yawn_state:
                            yawn_state = True
                            yawn_count += 1
                    else:
                        yawn_state = False

                    # --- ì‹œì„  ì´íƒˆ ê³„ì‚° ---
                    left_center = (landmarks[33].x + landmarks[133].x)/2
                    right_center = (landmarks[362].x + landmarks[263].x)/2
                    eye_center_x = (left_center + right_center)/2
                    if not (0.5 - GAZE_THRESHOLD <= eye_center_x <= 0.5 + GAZE_THRESHOLD):
                        gaze_out_seconds += 1/30.0

                # -------------------------------
                # 10ì´ˆ ë‹¨ìœ„ ì§‘ì¤‘ë„ ì˜ˆì¸¡ ë° ë°ì´í„° ëˆ„ì 
                # -------------------------------
                if time.time() - segment_start >= 10:
                    
                    # ğŸ’¡ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„
                    if model is not None:
                        features = np.array([[blink_count, yawn_count, closed_seconds, half_closed_seconds, gaze_out_seconds]])
                        score_pred = model.predict(features)[0]
                    else:
                        # ëª¨ë¸ ì—†ì„ ì‹œ ì„ì‹œ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜ ì‚¬ìš©
                        score_pred = calculate_focus(yawn_count, blink_count, closed_seconds, half_closed_seconds, gaze_out_seconds)


                    st.session_state['all_scores'].append(score_pred)
                    st.session_state['all_states'].append({
                        'blink_count': blink_count,
                        'yawn_count': yawn_count,
                        'closed_seconds': closed_seconds,
                        'half_closed_seconds': half_closed_seconds,
                        'gaze_out_seconds': gaze_out_seconds
                    })

                    # ì´ˆê¸°í™”
                    blink_count = yawn_count = closed_seconds = half_closed_seconds = gaze_out_seconds = 0
                    segment_start = time.time()
                
                # Streamlit Session Stateì— ë³€ìˆ˜ ì €ì¥
                st.session_state['blink_count'] = blink_count
                st.session_state['yawn_count'] = yawn_count
                st.session_state['closed_seconds'] = closed_seconds
                st.session_state['half_closed_seconds'] = half_closed_seconds
                st.session_state['gaze_out_seconds'] = gaze_out_seconds
                st.session_state['eye_closed'] = eye_closed
                st.session_state['yawn_state'] = yawn_state
                st.session_state['segment_start'] = segment_start
                
        finally:
            if cap is not None:
                cap.release()
            print("ì¹´ë©”ë¼ ë° ìŠ¤ë ˆë“œ ì¢…ë£Œ.")

# -------------------------------
# streamlit ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
# ------------------------------- 
if 'recording' not in st.session_state:
    st.session_state['recording'] = False
if 'recording_thread' not in st.session_state:
    st.session_state['recording_thread'] = None
if 'stop_flag' not in st.session_state:
    st.session_state['stop_flag'] = threading.Event()
if 'cap' not in st.session_state:
    st.session_state['cap'] = None

# -------------------------------
# ì¸¡ì • ë³€ìˆ˜ ì´ˆê¸°í™”
# ------------------------------- 
if 'all_scores' not in st.session_state:
    st.session_state['all_scores'] = []
if 'all_states' not in st.session_state:
    st.session_state['all_states'] = []

if 'segment_start' not in st.session_state:
    st.session_state['segment_start'] = time.time()
if 'blink_count' not in st.session_state:
    st.session_state['blink_count'] = 0
if 'yawn_count' not in st.session_state:
    st.session_state['yawn_count'] = 0
if 'closed_seconds' not in st.session_state:
    st.session_state['closed_seconds'] = 0
if 'half_closed_seconds' not in st.session_state:
    st.session_state['half_closed_seconds'] = 0
if 'gaze_out_seconds' not in st.session_state:
    st.session_state['gaze_out_seconds'] = 0
if 'eye_closed' not in st.session_state:
    st.session_state['eye_closed'] = False
if 'yawn_state' not in st.session_state:
    st.session_state['yawn_state'] = False
if 'last_analysis' not in st.session_state: # ìµœì¢… ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•  ê³µê°„
    st.session_state['last_analysis'] = None

st.title("ê³µë¶€ ì§‘ì¤‘ë„ í†µê³„")
st.header("ê³µë¶€ ì§‘ì¤‘ë„ í†µê³„")
st.caption("ìì‹ ì˜ ê³µë¶€ ì‹œê°„ì„ ê¸°ë¡í•˜ê³  ì§‘ì¤‘ë„ í†µê³„ ì‚°ì¶œí•˜ê³  ê³µë¶€ ìŠµê´€ì„ ì ê²€í•´ë³´ì„¸ìš”!")

# 2. ë²„íŠ¼ í´ë¦­ ì‹œ í˜¸ì¶œë  í•¨ìˆ˜ ì •ì˜
def toggle_recording():
    # í˜„ì¬ ìƒíƒœë¥¼ ë°˜ì „ì‹œí‚µë‹ˆë‹¤. (False -> True ë˜ëŠ” True -> False)
    st.session_state['recording'] = not st.session_state['recording']

    if st.session_state['recording']:
        # --- ë…¹í™” ì‹œì‘ ë¡œì§ ---
        st.session_state['cap'] = cv2.VideoCapture(0)
        
        if not st.session_state['cap'].isOpened():
            st.error("ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.session_state['recording'] = False # ì‹¤íŒ¨ ì‹œ ìƒíƒœ ì›ë³µ
            return
        
        # Stop í”Œë˜ê·¸ ì´ˆê¸°í™”
        st.session_state['stop_flag'].clear()

        # ë³„ë„ì˜ ìŠ¤ë ˆë“œì—ì„œ run_camera í•¨ìˆ˜ ì‹¤í–‰
        st.session_state['recording_thread'] = threading.Thread(
            target=statistics_start, 
            args=(st.session_state['cap'], st.session_state['stop_flag'])
        )
        st.session_state['recording_thread'].start()
        
    else:
        # --- ë…¹í™” ì¤‘ì§€ ë¡œì§ ---
        if st.session_state['recording_thread'] is not None and st.session_state['recording_thread'].is_alive():
            # ìŠ¤ë ˆë“œì— ì¢…ë£Œ ì‹ í˜¸ ì „ë‹¬
            st.session_state['stop_flag'].set()
            # ìŠ¤ë ˆë“œê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
            st.session_state['recording_thread'].join()
            st.session_state['recording_thread'] = None

# 3. ë²„íŠ¼ í‘œì‹œ
# ë²„íŠ¼ì˜ labelì„ í˜„ì¬ ìƒíƒœì— ë”°ë¼ ë³€ê²½í•©ë‹ˆë‹¤.
button_label = "í†µê³„ ì¸¡ì • ì¤‘ì§€" if st.session_state['recording'] else "í†µê³„ ì¸¡ì • ì‹œì‘"
st.button(button_label, on_click=toggle_recording)

# 4. ìƒíƒœì— ë”°ë¥¸ í…ìŠ¤íŠ¸ í‘œì‹œ
if st.session_state['recording']:
    st.success("ğŸ”´ ë…¹í™”ì¤‘...") # st.successëŠ” ë…¹ìƒ‰ ë°°ê²½ì˜ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.
else:
    # ë…¹í™” ì¤‘ì´ ì•„ë‹ ë•ŒëŠ” ì‹œì‘ ì•ˆë‚´ ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    st.info("ì¸¡ì •ì„ ì‹œì‘í•˜ë ¤ë©´ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")